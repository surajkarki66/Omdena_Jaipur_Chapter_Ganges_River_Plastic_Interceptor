{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
        "\n",
        "# Fine-Tune Segment Anything 2.1 (SAM-2.1)\n",
        "\n",
        "---\n",
        "\n",
        "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/facebookresearch/sam2)\n",
        "\n",
        "Segment Anything Model is a computer vision from Meta AI that can \"cut out\" any object, in any image, with a single click.\n",
        "\n",
        "In September 2024, Meta Research released SAM-2.1 SAM-2.1 is the latest model in the Segment Anything model series. When evaluated against the Segment Anything V test set, the MOSE validation set, and the LVOSv2 dataset, all SAM-2.1 model sizes perform better than SAM-2.\n",
        "\n",
        "SAM-2.1 was released with training instructions that you can use to fine-tune SAM-2.1 for a specific use case. This is ideal if you want to train SAM-2.1 to segment objects in a specific domain at which the base model struggles.\n",
        "\n",
        "Here is an example of results from SAM-2, sourced from the Meta SAM-2 GitHub repository:\n",
        "\n",
        "![segment anything model](https://github.com/facebookresearch/sam2/raw/main/assets/sa_v_dataset.jpg?raw=true)\n",
        "\n",
        "We recommend that you follow along in this notebook while reading the blog post on [SAM-2.1 fine-tuning](https://blog.roboflow.com/sam-2-1-fine-tuning).\n",
        "\n",
        "## Pro Tip: Use GPU Acceleration\n",
        "\n",
        "If you are running this notebook in Google Colab, navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`. This will ensure your notebook uses a GPU, which will significantly speed up model training times.\n",
        "\n",
        "Meta recommends training SAM-2.1 on an A100. Thus, if possible, select an A100 GPU in Google Colab for use in training the model.\n",
        "\n",
        "## Steps in this Tutorial\n",
        "\n",
        "In this tutorial, we are going to cover:\n",
        "\n",
        "- **Before you start** - Make sure you have access to the GPU\n",
        "- Download SAM-2.1\n",
        "- Download Example Data\n",
        "- Load Model\n",
        "- Automated Mask Generation\n",
        "\n",
        "Without further ado, let's get started!"
      ],
      "metadata": {
        "id": "-AEkJ13euBJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download SAM-2.1 and Data\n",
        "\n",
        "Below, we download SAM-2.1 from GitHub, then download a dataset for use in training. You will need a dataset structured in the correct format for SAM-2.1.\n",
        "\n",
        "Roboflow supports exporting segmentation datasets to the SAM-2.1 format, ideal for use in this guide. You can upload segmentation datasets in the COCO JSON Segmentation format then convert them to SAM-2.1 for use in this guide.\n",
        "\n",
        "[Learn how to label a dataset in Roboflow](https://blog.roboflow.com/getting-started-with-roboflow/)\n",
        "\n",
        "[Learn how to export data from Roboflow for training](https://docs.roboflow.com/datasets/exporting-data).\n",
        "\n",
        "![Export as SAM-2 data](https://media.roboflow.com/sam2export.png)\n",
        "\n",
        "We then download a SAM-2.1 training YAML file which we will use to configure our model training job.\n",
        "\n",
        "Finally, we install SAM-2.1 and download the model checkpoints.\n",
        "\n",
        "Replace the below code with the code to export your dataset. You can also use the same code above to fine-tune our car parts dataset. _Note: If you use the car parts dataset pre-filled below, you will still need to add a [Roboflow API key](https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key)._\n",
        "\n",
        "**Important ⚠️: You must generate a dataset with images stretched to 1024x1024 for training. This is because our training configuration is set to use images of this resolution.**"
      ],
      "metadata": {
        "id": "xEX6u7N2uecP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "import os\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"U37Y1VWgripWggOxgiCE\")\n",
        "project = rf.workspace(\"project-zero\").project(\"aerial_river_plastic_wastes\")\n",
        "version = project.version(9)\n",
        "dataset = version.download(\"sam2\")\n",
        "\n",
        "# rename dataset.location to \"data\"\n",
        "os.rename(dataset.location, \"/content/data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbDFNKNDw6Pq",
        "outputId": "6d2b4410-8781-4119-88b2-203787b2f306",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.50-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from roboflow) (2024.8.30)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.26.4)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (11.0.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.2.3)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.6)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.3.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.55.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.4.0)\n",
            "Downloading roboflow-1.1.50-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: filetype, python-dotenv, idna, roboflow\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "Successfully installed filetype-1.2.0 idna-3.7 python-dotenv-1.0.1 roboflow-1.1.50\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Aerial_River_Plastic_Wastes-9 to sam2:: 100%|██████████| 134879/134879 [00:08<00:00, 15795.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Aerial_River_Plastic_Wastes-9 in sam2:: 100%|██████████| 4931/4931 [00:01<00:00, 3128.24it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex6NHq0bMucS",
        "outputId": "ed2a38dd-adfa-4b3a-b65d-4222c118a387"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'sam2'...\n",
            "remote: Enumerating objects: 1052, done.\u001b[K\n",
            "remote: Counting objects: 100% (460/460), done.\u001b[K\n",
            "remote: Compressing objects: 100% (205/205), done.\u001b[K\n",
            "remote: Total 1052 (delta 277), reused 290 (delta 255), pack-reused 592 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1052/1052), 121.74 MiB | 1.36 MiB/s, done.\n",
            "Resolving deltas: 100% (378/378), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/facebookresearch/sam2.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O /content/sam2/sam2/configs/train.yaml 'https://drive.usercontent.google.com/download?id=11cmbxPPsYqFyWq87tmLgBAQ6OZgEhPG3'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEAfWyHxE2ob",
        "outputId": "f4fc5d46-fa51-4243-c848-68e174fc604b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-14 12:13:11--  https://drive.usercontent.google.com/download?id=11cmbxPPsYqFyWq87tmLgBAQ6OZgEhPG3\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.143.132, 2a00:1450:4013:c03::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.143.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11055 (11K) [application/octet-stream]\n",
            "Saving to: ‘/content/sam2/sam2/configs/train.yaml’\n",
            "\n",
            "/content/sam2/sam2/ 100%[===================>]  10.80K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-14 12:13:13 (85.6 MB/s) - ‘/content/sam2/sam2/configs/train.yaml’ saved [11055/11055]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ./sam2/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBrOMeWm4ZAW",
        "outputId": "99bb10c6-575f-4e2c-a5ca-5d7b21810b36"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/sam2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we are going to install SAM-2.\n",
        "\n",
        "The SAM-2 installation process may take several minutes."
      ],
      "metadata": {
        "id": "Pak3HIdXvi-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e .[dev] -q"
      ],
      "metadata": {
        "id": "mTFQSONnNlRx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ee61ba0-4422-4508-c30f-b5d8c429d6d8",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.9/74.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m359.9/359.9 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building editable for SAM-2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./checkpoints && ./download_ckpts.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJZGcpRpgevM",
        "outputId": "f60d7ba0-9fd4-41c0-c083-2cd9aefebb8d",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading sam2.1_hiera_tiny.pt checkpoint...\n",
            "--2024-12-14 12:17:46--  https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_tiny.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.227.219.33, 13.227.219.10, 13.227.219.59, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.227.219.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 156008466 (149M) [application/vnd.snesdev-page-table]\n",
            "Saving to: ‘sam2.1_hiera_tiny.pt’\n",
            "\n",
            "sam2.1_hiera_tiny.p 100%[===================>] 148.78M   229MB/s    in 0.6s    \n",
            "\n",
            "2024-12-14 12:17:47 (229 MB/s) - ‘sam2.1_hiera_tiny.pt’ saved [156008466/156008466]\n",
            "\n",
            "Downloading sam2.1_hiera_small.pt checkpoint...\n",
            "--2024-12-14 12:17:47--  https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_small.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.227.219.33, 13.227.219.10, 13.227.219.59, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.227.219.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 184416285 (176M) [application/vnd.snesdev-page-table]\n",
            "Saving to: ‘sam2.1_hiera_small.pt’\n",
            "\n",
            "sam2.1_hiera_small. 100%[===================>] 175.87M   270MB/s    in 0.7s    \n",
            "\n",
            "2024-12-14 12:17:48 (270 MB/s) - ‘sam2.1_hiera_small.pt’ saved [184416285/184416285]\n",
            "\n",
            "Downloading sam2.1_hiera_base_plus.pt checkpoint...\n",
            "--2024-12-14 12:17:48--  https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_base_plus.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.227.219.33, 13.227.219.10, 13.227.219.59, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.227.219.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 323606802 (309M) [application/vnd.snesdev-page-table]\n",
            "Saving to: ‘sam2.1_hiera_base_plus.pt’\n",
            "\n",
            "sam2.1_hiera_base_p 100%[===================>] 308.62M   231MB/s    in 1.3s    \n",
            "\n",
            "2024-12-14 12:17:49 (231 MB/s) - ‘sam2.1_hiera_base_plus.pt’ saved [323606802/323606802]\n",
            "\n",
            "Downloading sam2.1_hiera_large.pt checkpoint...\n",
            "--2024-12-14 12:17:49--  https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.227.219.33, 13.227.219.10, 13.227.219.59, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.227.219.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 898083611 (856M) [application/vnd.snesdev-page-table]\n",
            "Saving to: ‘sam2.1_hiera_large.pt’\n",
            "\n",
            "sam2.1_hiera_large. 100%[===================>] 856.48M  4.36MB/s    in 8.2s    \n",
            "\n",
            "2024-12-14 12:17:57 (104 MB/s) - ‘sam2.1_hiera_large.pt’ saved [898083611/898083611]\n",
            "\n",
            "All checkpoints are downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "C02KdmNaSg8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modify Dataset File Names\n",
        "\n",
        "SAM-2.1 requires dataset file names to be in a particular format. Run the code snippet below to format your dataset file names as required."
      ],
      "metadata": {
        "id": "HuO41RTpuz_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Script to rename roboflow filenames to something SAM 2.1 compatible.\n",
        "# Maybe it is possible to remove this step tweaking sam2/sam2/configs/train.yaml.\n",
        "import os\n",
        "import re\n",
        "\n",
        "FOLDER = \"/content/data/train\"\n",
        "\n",
        "for filename in os.listdir(FOLDER):\n",
        "    # Replace all except last dot with underscore\n",
        "    new_filename = filename.replace(\".\", \"_\", filename.count(\".\") - 1)\n",
        "    if not re.search(r\"_\\d+\\.\\w+$\", new_filename):\n",
        "        # Add an int to the end of base name\n",
        "        new_filename = new_filename.replace(\".\", \"_1.\")\n",
        "    os.rename(os.path.join(FOLDER, filename), os.path.join(FOLDER, new_filename))"
      ],
      "metadata": {
        "id": "9Qj9S3CmBGLK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Training\n",
        "\n",
        "You can now start training a SAM-2.1 model. The amount of time it will take to train the model will vary depending on the GPU you are using and the number of images in your dataset.\n",
        "\n",
        "For the car part dataset of 38 images, training on an A100 GPU takes ~15 minutes."
      ],
      "metadata": {
        "id": "aFYUuiL8u7KI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python training/train.py -c 'configs/train.yaml' --use-cluster 0 --num-gpus 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtdNF84fSA40",
        "outputId": "59626389-bebf-4b1b-833f-b57a14d63431",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###################### Train App Config ####################\n",
            "scratch:\n",
            "  resolution: 1024\n",
            "  train_batch_size: 1\n",
            "  num_train_workers: 10\n",
            "  num_frames: 1\n",
            "  max_num_objects: 3\n",
            "  base_lr: 5.0e-06\n",
            "  vision_lr: 3.0e-06\n",
            "  phases_per_epoch: 1\n",
            "  num_epochs: 40\n",
            "dataset:\n",
            "  img_folder: /content/data/train\n",
            "  gt_folder: /content/data/train\n",
            "  multiplier: 2\n",
            "vos:\n",
            "  train_transforms:\n",
            "  - _target_: training.dataset.transforms.ComposeAPI\n",
            "    transforms:\n",
            "    - _target_: training.dataset.transforms.RandomHorizontalFlip\n",
            "      consistent_transform: true\n",
            "    - _target_: training.dataset.transforms.RandomAffine\n",
            "      degrees: 25\n",
            "      shear: 20\n",
            "      image_interpolation: bilinear\n",
            "      consistent_transform: true\n",
            "    - _target_: training.dataset.transforms.RandomResizeAPI\n",
            "      sizes: ${scratch.resolution}\n",
            "      square: true\n",
            "      consistent_transform: true\n",
            "    - _target_: training.dataset.transforms.ColorJitter\n",
            "      consistent_transform: true\n",
            "      brightness: 0.1\n",
            "      contrast: 0.03\n",
            "      saturation: 0.03\n",
            "      hue: null\n",
            "    - _target_: training.dataset.transforms.RandomGrayscale\n",
            "      p: 0.05\n",
            "      consistent_transform: true\n",
            "    - _target_: training.dataset.transforms.ColorJitter\n",
            "      consistent_transform: false\n",
            "      brightness: 0.1\n",
            "      contrast: 0.05\n",
            "      saturation: 0.05\n",
            "      hue: null\n",
            "    - _target_: training.dataset.transforms.ToTensorAPI\n",
            "    - _target_: training.dataset.transforms.NormalizeAPI\n",
            "      mean:\n",
            "      - 0.485\n",
            "      - 0.456\n",
            "      - 0.406\n",
            "      std:\n",
            "      - 0.229\n",
            "      - 0.224\n",
            "      - 0.225\n",
            "trainer:\n",
            "  _target_: training.trainer.Trainer\n",
            "  mode: train_only\n",
            "  max_epochs: ${times:${scratch.num_epochs},${scratch.phases_per_epoch}}\n",
            "  accelerator: cuda\n",
            "  seed_value: 123\n",
            "  model:\n",
            "    _target_: training.model.sam2.SAM2Train\n",
            "    image_encoder:\n",
            "      _target_: sam2.modeling.backbones.image_encoder.ImageEncoder\n",
            "      scalp: 1\n",
            "      trunk:\n",
            "        _target_: sam2.modeling.backbones.hieradet.Hiera\n",
            "        embed_dim: 112\n",
            "        num_heads: 2\n",
            "        drop_path_rate: 0.1\n",
            "      neck:\n",
            "        _target_: sam2.modeling.backbones.image_encoder.FpnNeck\n",
            "        position_encoding:\n",
            "          _target_: sam2.modeling.position_encoding.PositionEmbeddingSine\n",
            "          num_pos_feats: 256\n",
            "          normalize: true\n",
            "          scale: null\n",
            "          temperature: 10000\n",
            "        d_model: 256\n",
            "        backbone_channel_list:\n",
            "        - 896\n",
            "        - 448\n",
            "        - 224\n",
            "        - 112\n",
            "        fpn_top_down_levels:\n",
            "        - 2\n",
            "        - 3\n",
            "        fpn_interp_model: nearest\n",
            "    memory_attention:\n",
            "      _target_: sam2.modeling.memory_attention.MemoryAttention\n",
            "      d_model: 256\n",
            "      pos_enc_at_input: true\n",
            "      layer:\n",
            "        _target_: sam2.modeling.memory_attention.MemoryAttentionLayer\n",
            "        activation: relu\n",
            "        dim_feedforward: 2048\n",
            "        dropout: 0.1\n",
            "        pos_enc_at_attn: false\n",
            "        self_attention:\n",
            "          _target_: sam2.modeling.sam.transformer.RoPEAttention\n",
            "          rope_theta: 10000.0\n",
            "          feat_sizes:\n",
            "          - 32\n",
            "          - 32\n",
            "          embedding_dim: 256\n",
            "          num_heads: 1\n",
            "          downsample_rate: 1\n",
            "          dropout: 0.1\n",
            "        d_model: 256\n",
            "        pos_enc_at_cross_attn_keys: true\n",
            "        pos_enc_at_cross_attn_queries: false\n",
            "        cross_attention:\n",
            "          _target_: sam2.modeling.sam.transformer.RoPEAttention\n",
            "          rope_theta: 10000.0\n",
            "          feat_sizes:\n",
            "          - 32\n",
            "          - 32\n",
            "          rope_k_repeat: true\n",
            "          embedding_dim: 256\n",
            "          num_heads: 1\n",
            "          downsample_rate: 1\n",
            "          dropout: 0.1\n",
            "          kv_in_dim: 64\n",
            "      num_layers: 4\n",
            "    memory_encoder:\n",
            "      _target_: sam2.modeling.memory_encoder.MemoryEncoder\n",
            "      out_dim: 64\n",
            "      position_encoding:\n",
            "        _target_: sam2.modeling.position_encoding.PositionEmbeddingSine\n",
            "        num_pos_feats: 64\n",
            "        normalize: true\n",
            "        scale: null\n",
            "        temperature: 10000\n",
            "      mask_downsampler:\n",
            "        _target_: sam2.modeling.memory_encoder.MaskDownSampler\n",
            "        kernel_size: 3\n",
            "        stride: 2\n",
            "        padding: 1\n",
            "      fuser:\n",
            "        _target_: sam2.modeling.memory_encoder.Fuser\n",
            "        layer:\n",
            "          _target_: sam2.modeling.memory_encoder.CXBlock\n",
            "          dim: 256\n",
            "          kernel_size: 7\n",
            "          padding: 3\n",
            "          layer_scale_init_value: 1.0e-06\n",
            "          use_dwconv: true\n",
            "        num_layers: 2\n",
            "    num_maskmem: 7\n",
            "    image_size: ${scratch.resolution}\n",
            "    sigmoid_scale_for_mem_enc: 20.0\n",
            "    sigmoid_bias_for_mem_enc: -10.0\n",
            "    use_mask_input_as_output_without_sam: true\n",
            "    directly_add_no_mem_embed: true\n",
            "    no_obj_embed_spatial: true\n",
            "    use_high_res_features_in_sam: true\n",
            "    multimask_output_in_sam: true\n",
            "    iou_prediction_use_sigmoid: true\n",
            "    use_obj_ptrs_in_encoder: true\n",
            "    add_tpos_enc_to_obj_ptrs: true\n",
            "    proj_tpos_enc_in_obj_ptrs: true\n",
            "    use_signed_tpos_enc_to_obj_ptrs: true\n",
            "    only_obj_ptrs_in_the_past_for_eval: true\n",
            "    pred_obj_scores: true\n",
            "    pred_obj_scores_mlp: true\n",
            "    fixed_no_obj_ptr: true\n",
            "    multimask_output_for_tracking: true\n",
            "    use_multimask_token_for_obj_ptr: true\n",
            "    multimask_min_pt_num: 0\n",
            "    multimask_max_pt_num: 1\n",
            "    use_mlp_for_obj_ptr_proj: true\n",
            "    prob_to_use_pt_input_for_train: 0.5\n",
            "    prob_to_use_pt_input_for_eval: 0.0\n",
            "    prob_to_use_box_input_for_train: 0.5\n",
            "    prob_to_use_box_input_for_eval: 0.0\n",
            "    prob_to_sample_from_gt_for_train: 0.1\n",
            "    num_frames_to_correct_for_train: 2\n",
            "    num_frames_to_correct_for_eval: 1\n",
            "    rand_frames_to_correct_for_train: true\n",
            "    add_all_frames_to_correct_as_cond: true\n",
            "    num_init_cond_frames_for_train: 2\n",
            "    rand_init_cond_frames_for_train: true\n",
            "    num_correction_pt_per_frame: 7\n",
            "    use_act_ckpt_iterative_pt_sampling: false\n",
            "    num_init_cond_frames_for_eval: 1\n",
            "    forward_backbone_per_frame_for_eval: true\n",
            "  data:\n",
            "    train:\n",
            "      _target_: training.dataset.sam2_datasets.TorchTrainMixedDataset\n",
            "      phases_per_epoch: ${scratch.phases_per_epoch}\n",
            "      batch_sizes:\n",
            "      - ${scratch.train_batch_size}\n",
            "      datasets:\n",
            "      - _target_: training.dataset.vos_dataset.VOSDataset\n",
            "        transforms: ${vos.train_transforms}\n",
            "        training: true\n",
            "        video_dataset:\n",
            "          _target_: training.dataset.vos_raw_dataset.SA1BRawDataset\n",
            "          img_folder: ${dataset.img_folder}\n",
            "          gt_folder: ${dataset.gt_folder}\n",
            "        multiplier: ${dataset.multiplier}\n",
            "        sampler:\n",
            "          _target_: training.dataset.vos_sampler.RandomUniformSampler\n",
            "          num_frames: 1\n",
            "          max_num_objects: ${scratch.max_num_objects}\n",
            "      shuffle: true\n",
            "      num_workers: ${scratch.num_train_workers}\n",
            "      pin_memory: true\n",
            "      drop_last: true\n",
            "      collate_fn:\n",
            "        _target_: training.utils.data_utils.collate_fn\n",
            "        _partial_: true\n",
            "        dict_key: all\n",
            "  optim:\n",
            "    amp:\n",
            "      enabled: true\n",
            "      amp_dtype: bfloat16\n",
            "    optimizer:\n",
            "      _target_: torch.optim.AdamW\n",
            "    gradient_clip:\n",
            "      _target_: training.optimizer.GradientClipper\n",
            "      max_norm: 0.1\n",
            "      norm_type: 2\n",
            "    param_group_modifiers:\n",
            "    - _target_: training.optimizer.layer_decay_param_modifier\n",
            "      _partial_: true\n",
            "      layer_decay_value: 0.9\n",
            "      apply_to: image_encoder.trunk\n",
            "      overrides:\n",
            "      - pattern: '*pos_embed*'\n",
            "        value: 1.0\n",
            "    options:\n",
            "      lr:\n",
            "      - scheduler:\n",
            "          _target_: fvcore.common.param_scheduler.CosineParamScheduler\n",
            "          start_value: ${scratch.base_lr}\n",
            "          end_value: ${divide:${scratch.base_lr},10}\n",
            "      - scheduler:\n",
            "          _target_: fvcore.common.param_scheduler.CosineParamScheduler\n",
            "          start_value: ${scratch.vision_lr}\n",
            "          end_value: ${divide:${scratch.vision_lr},10}\n",
            "        param_names:\n",
            "        - image_encoder.*\n",
            "      weight_decay:\n",
            "      - scheduler:\n",
            "          _target_: fvcore.common.param_scheduler.ConstantParamScheduler\n",
            "          value: 0.1\n",
            "      - scheduler:\n",
            "          _target_: fvcore.common.param_scheduler.ConstantParamScheduler\n",
            "          value: 0.0\n",
            "        param_names:\n",
            "        - '*bias*'\n",
            "        module_cls_names:\n",
            "        - torch.nn.LayerNorm\n",
            "  loss:\n",
            "    all:\n",
            "      _target_: training.loss_fns.MultiStepMultiMasksAndIous\n",
            "      weight_dict:\n",
            "        loss_mask: 20\n",
            "        loss_dice: 1\n",
            "        loss_iou: 1\n",
            "        loss_class: 1\n",
            "      supervise_all_iou: true\n",
            "      iou_use_l1_loss: true\n",
            "      pred_obj_scores: true\n",
            "      focal_gamma_obj_score: 0.0\n",
            "      focal_alpha_obj_score: -1.0\n",
            "  distributed:\n",
            "    backend: nccl\n",
            "    find_unused_parameters: true\n",
            "  logging:\n",
            "    tensorboard_writer:\n",
            "      _target_: training.utils.logger.make_tensorboard_logger\n",
            "      log_dir: ${launcher.experiment_log_dir}/tensorboard\n",
            "      flush_secs: 120\n",
            "      should_log: true\n",
            "    log_dir: ${launcher.experiment_log_dir}/logs\n",
            "    log_freq: 10\n",
            "  checkpoint:\n",
            "    save_dir: ${launcher.experiment_log_dir}/checkpoints\n",
            "    save_freq: 0\n",
            "    model_weight_initializer:\n",
            "      _partial_: true\n",
            "      _target_: training.utils.checkpoint_utils.load_state_dict_into_model\n",
            "      strict: true\n",
            "      ignore_unexpected_keys: null\n",
            "      ignore_missing_keys: null\n",
            "      state_dict:\n",
            "        _target_: training.utils.checkpoint_utils.load_checkpoint_and_apply_kernels\n",
            "        checkpoint_path: ./checkpoints/sam2.1_hiera_base_plus.pt\n",
            "        ckpt_state_dict_keys:\n",
            "        - model\n",
            "launcher:\n",
            "  num_nodes: 1\n",
            "  gpus_per_node: 8\n",
            "  experiment_log_dir: /content/sam2/sam2_logs/configs/train.yaml\n",
            "submitit:\n",
            "  partition: null\n",
            "  account: null\n",
            "  qos: null\n",
            "  cpus_per_task: 10\n",
            "  use_cluster: false\n",
            "  timeout_hour: 24\n",
            "  name: null\n",
            "  port_range:\n",
            "  - 10000\n",
            "  - 65000\n",
            "\n",
            "############################################################\n",
            "2024-12-14 12:18:10.475486: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-14 12:18:10.509355: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-14 12:18:10.519742: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-14 12:18:10.543963: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-14 12:18:12.588528: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO 2024-12-14 12:18:14,443 train_utils.py: 108: MACHINE SEED: 4920\n",
            "INFO 2024-12-14 12:18:14,448 train_utils.py: 154: Logging ENV_VARIABLES\n",
            "INFO 2024-12-14 12:18:14,448 train_utils.py: 155: CGROUP_MEMORY_EVENTS=/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events\n",
            "CLICOLOR=1\n",
            "CLOUDSDK_CONFIG=/content/.config\n",
            "CLOUDSDK_PYTHON=python3\n",
            "COLAB_BACKEND_VERSION=next\n",
            "COLAB_DEBUG_ADAPTER_MUX_PATH=/usr/local/bin/dap_multiplexer\n",
            "COLAB_FILE_HANDLER_ADDR=localhost:3453\n",
            "COLAB_GPU=1\n",
            "COLAB_JUPYTER_IP=172.28.0.12\n",
            "COLAB_JUPYTER_TOKEN=\n",
            "COLAB_JUPYTER_TRANSPORT=ipc\n",
            "COLAB_KERNEL_MANAGER_PROXY_HOST=172.28.0.12\n",
            "COLAB_KERNEL_MANAGER_PROXY_PORT=6000\n",
            "COLAB_LANGUAGE_SERVER_PROXY=/usr/colab/bin/language_service\n",
            "COLAB_LANGUAGE_SERVER_PROXY_LSP_DIRS=/datalab/web/pyright/typeshed-fallback/stdlib,/usr/local/lib/python3.10/dist-packages\n",
            "COLAB_LANGUAGE_SERVER_PROXY_REQUEST_TIMEOUT=30s\n",
            "COLAB_LANGUAGE_SERVER_PROXY_ROOT_URL=http://172.28.0.1:8013/\n",
            "COLAB_RELEASE_TAG=release-colab_20241212-060129_RC00\n",
            "COLAB_TPU_1VM=\n",
            "COLAB_WARMUP_DEFAULTS=1\n",
            "COLUMNS=100\n",
            "CUDA_MODULE_LOADING=LAZY\n",
            "CUDA_VERSION=12.2.2\n",
            "DEBIAN_FRONTEND=noninteractive\n",
            "ENABLE_DIRECTORYPREFETCHER=1\n",
            "ENV=/root/.bashrc\n",
            "GCE_METADATA_TIMEOUT=3\n",
            "GCS_READ_CACHE_BLOCK_SIZE_MB=16\n",
            "GIT_PAGER=cat\n",
            "HOME=/root\n",
            "HOSTNAME=ae65b7cd96c1\n",
            "HYDRA_FULL_ERROR=1\n",
            "JPY_PARENT_PID=114\n",
            "KMP_EXTRA_ARGS=--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https://colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-37sfqpa4yfb6z --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true --enable_kernel_event_logging=true\n",
            "KMP_LISTEN_PORT=6000\n",
            "KMP_TARGET_PORT=9000\n",
            "LANG=en_US.UTF-8\n",
            "LANGUAGE=en_US\n",
            "LAST_FORCED_REBUILD=20241204\n",
            "LC_ALL=en_US.UTF-8\n",
            "LD_LIBRARY_PATH=/usr/lib64-nvidia\n",
            "LIBRARY_PATH=/usr/local/cuda/lib64/stubs\n",
            "LOCAL_RANK=0\n",
            "MASTER_ADDR=localhost\n",
            "MASTER_PORT=43220\n",
            "MPLBACKEND=module://ipykernel.pylab.backend_inline\n",
            "NCCL_VERSION=2.19.3-1\n",
            "NO_GCE_CHECK=False\n",
            "NVARCH=x86_64\n",
            "NVIDIA_DRIVER_CAPABILITIES=compute,utility\n",
            "NVIDIA_PRODUCT_NAME=CUDA\n",
            "NVIDIA_REQUIRE_CUDA=cuda>=12.2 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526\n",
            "NVIDIA_VISIBLE_DEVICES=all\n",
            "NV_CUDA_COMPAT_PACKAGE=cuda-compat-12-2\n",
            "NV_CUDA_CUDART_DEV_VERSION=12.2.140-1\n",
            "NV_CUDA_CUDART_VERSION=12.2.140-1\n",
            "NV_CUDA_LIB_VERSION=12.2.2-1\n",
            "NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE=cuda-nsight-compute-12-2=12.2.2-1\n",
            "NV_CUDA_NSIGHT_COMPUTE_VERSION=12.2.2-1\n",
            "NV_CUDNN_PACKAGE=libcudnn8=8.9.6.50-1+cuda12.2\n",
            "NV_CUDNN_PACKAGE_DEV=libcudnn8-dev=8.9.6.50-1+cuda12.2\n",
            "NV_CUDNN_PACKAGE_NAME=libcudnn8\n",
            "NV_CUDNN_VERSION=8.9.6.50\n",
            "NV_LIBCUBLAS_DEV_PACKAGE=libcublas-dev-12-2=12.2.5.6-1\n",
            "NV_LIBCUBLAS_DEV_PACKAGE_NAME=libcublas-dev-12-2\n",
            "NV_LIBCUBLAS_DEV_VERSION=12.2.5.6-1\n",
            "NV_LIBCUBLAS_PACKAGE=libcublas-12-2=12.2.5.6-1\n",
            "NV_LIBCUBLAS_PACKAGE_NAME=libcublas-12-2\n",
            "NV_LIBCUBLAS_VERSION=12.2.5.6-1\n",
            "NV_LIBCUSPARSE_DEV_VERSION=12.1.2.141-1\n",
            "NV_LIBCUSPARSE_VERSION=12.1.2.141-1\n",
            "NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.19.3-1+cuda12.2\n",
            "NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
            "NV_LIBNCCL_DEV_PACKAGE_VERSION=2.19.3-1\n",
            "NV_LIBNCCL_PACKAGE=libnccl2=2.19.3-1+cuda12.2\n",
            "NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
            "NV_LIBNCCL_PACKAGE_VERSION=2.19.3-1\n",
            "NV_LIBNPP_DEV_PACKAGE=libnpp-dev-12-2=12.2.1.4-1\n",
            "NV_LIBNPP_DEV_VERSION=12.2.1.4-1\n",
            "NV_LIBNPP_PACKAGE=libnpp-12-2=12.2.1.4-1\n",
            "NV_LIBNPP_VERSION=12.2.1.4-1\n",
            "NV_NVML_DEV_VERSION=12.2.140-1\n",
            "NV_NVPROF_DEV_PACKAGE=cuda-nvprof-12-2=12.2.142-1\n",
            "NV_NVPROF_VERSION=12.2.142-1\n",
            "NV_NVTX_VERSION=12.2.140-1\n",
            "OLDPWD=/\n",
            "PAGER=cat\n",
            "PATH=/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n",
            "PWD=/content/sam2\n",
            "PYDEVD_USE_FRAME_EVAL=NO\n",
            "PYTHONPATH=/env/python\n",
            "PYTHONWARNINGS=ignore:::pip._internal.cli.base_command\n",
            "RANK=0\n",
            "SHELL=/bin/bash\n",
            "SHLVL=0\n",
            "TBE_CREDS_ADDR=172.28.0.1:8008\n",
            "TBE_EPHEM_CREDS_ADDR=172.28.0.1:8009\n",
            "TBE_RUNTIME_ADDR=172.28.0.1:8011\n",
            "TCLLIBPATH=/usr/share/tcltk/tcllib1.20\n",
            "TERM=xterm-color\n",
            "TF2_BEHAVIOR=1\n",
            "TF_CPP_MIN_LOG_LEVEL=1\n",
            "TF_FORCE_GPU_ALLOW_GROWTH=true\n",
            "TORCH_NCCL_ASYNC_ERROR_HANDLING=1\n",
            "TPU_ML_PLATFORM=Tensorflow\n",
            "TPU_ML_PLATFORM_VERSION=2.17.1\n",
            "USE_AUTH_EPHEM=1\n",
            "VM_GCE_METADATA_HOST=169.254.169.253\n",
            "WORLD_SIZE=1\n",
            "_=/usr/local/bin/python\n",
            "\n",
            "INFO 2024-12-14 12:18:14,448 trainer.py: 989: Setting up components: Model, loss, optim, meters etc.\n",
            "INFO 2024-12-14 12:18:14,449 logger.py:  66: TensorBoard SummaryWriter instantiated. Files will be stored in: /content/sam2/sam2_logs/configs/train.yaml/tensorboard\n",
            "INFO 2024-12-14 12:18:15,761 sam2.py:  81: Training with points (sampled from masks) as inputs with p=0.5\n",
            "INFO 2024-12-14 12:18:15,764 trainer.py:1059: ====================\n",
            "INFO 2024-12-14 12:18:15,764 trainer.py:1060: Summary for model <class 'training.model.sam2.SAM2Train'>\n",
            "INFO 2024-12-14 12:18:15,767 trainer.py:1061: Model is SAM2Train(\n",
            "  (image_encoder): ImageEncoder(\n",
            "    (trunk): Hiera(\n",
            "      (patch_embed): PatchEmbed(\n",
            "        (proj): Conv2d(3, 112, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
            "      )\n",
            "      (blocks): ModuleList(\n",
            "        (0): MultiScaleBlock(\n",
            "          (norm1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): MultiScaleAttention(\n",
            "            (qkv): Linear(in_features=112, out_features=336, bias=True)\n",
            "            (proj): Linear(in_features=112, out_features=112, bias=True)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "          (norm2): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLP(\n",
            "            (layers): ModuleList(\n",
            "              (0): Linear(in_features=112, out_features=448, bias=True)\n",
            "              (1): Linear(in_features=448, out_features=112, bias=True)\n",
            "            )\n",
            "            (act): GELU(approximate='none')\n",
            "          )\n",
            "        )\n",
            "        (1): MultiScaleBlock(\n",
            "          (norm1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): MultiScaleAttention(\n",
            "            (qkv): Linear(in_features=112, out_features=336, bias=True)\n",
            "            (proj): Linear(in_features=112, out_features=112, bias=True)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLP(\n",
            "            (layers): ModuleList(\n",
            "              (0): Linear(in_features=112, out_features=448, bias=True)\n",
            "              (1): Linear(in_features=448, out_features=112, bias=True)\n",
            "            )\n",
            "            (act): GELU(approximate='none')\n",
            "          )\n",
            "        )\n",
            "        (2): MultiScaleBlock(\n",
            "          (norm1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n",
            "          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "          (attn): MultiScaleAttention(\n",
            "            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "            (qkv): Linear(in_features=112, out_features=672, bias=True)\n",
            "            (proj): Linear(in_features=224, out_features=224, bias=True)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((224,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLP(\n",
            "            (layers): ModuleList(\n",
            "              (0): Linear(in_features=224, out_features=896, bias=True)\n",
            "              (1): Linear(in_features=896, out_features=224, bias=True)\n",
            "            )\n",
            "            (act): GELU(approximate='none')\n",
            "          )\n",
            "          (proj): Linear(in_features=112, out_features=224, bias=True)\n",
            "        )\n",
            "        (3-4): 2 x MultiScaleBlock(\n",
            "          (norm1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): MultiScaleAttention(\n",
            "            (qkv): Linear(in_features=224, out_features=672, bias=True)\n",
            "            (proj): Linear(in_features=224, out_features=224, bias=True)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((224,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLP(\n",
            "            (layers): ModuleList(\n",
            "              (0): Linear(in_features=224, out_features=896, bias=True)\n",
            "              (1): Linear(in_features=896, out_features=224, bias=True)\n",
            "            )\n",
            "            (act): GELU(approximate='none')\n",
            "          )\n",
            "        )\n",
            "        (5): MultiScaleBlock(\n",
            "          (norm1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)\n",
            "          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "          (attn): MultiScaleAttention(\n",
            "            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "            (qkv): Linear(in_features=224, out_features=1344, bias=True)\n",
            "            (proj): Linear(in_features=448, out_features=448, bias=True)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((448,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLP(\n",
            "            (layers): ModuleList(\n",
            "              (0): Linear(in_features=448, out_features=1792, bias=True)\n",
            "              (1): Linear(in_features=1792, out_features=448, bias=True)\n",
            "            )\n",
            "            (act): GELU(approximate='none')\n",
            "          )\n",
            "          (proj): Linear(in_features=224, out_features=448, bias=True)\n",
            "        )\n",
            "        (6-20): 15 x MultiScaleBlock(\n",
            "          (norm1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): MultiScaleAttention(\n",
            "            (qkv): Linear(in_features=448, out_features=1344, bias=True)\n",
            "            (proj): Linear(in_features=448, out_features=448, bias=True)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((448,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLP(\n",
            "            (layers): ModuleList(\n",
            "              (0): Linear(in_features=448, out_features=1792, bias=True)\n",
            "              (1): Linear(in_features=1792, out_features=448, bias=True)\n",
            "            )\n",
            "            (act): GELU(approximate='none')\n",
            "          )\n",
            "        )\n",
            "        (21): MultiScaleBlock(\n",
            "          (norm1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)\n",
            "          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "          (attn): MultiScaleAttention(\n",
            "            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "            (qkv): Linear(in_features=448, out_features=2688, bias=True)\n",
            "            (proj): Linear(in_features=896, out_features=896, bias=True)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((896,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLP(\n",
            "            (layers): ModuleList(\n",
            "              (0): Linear(in_features=896, out_features=3584, bias=True)\n",
            "              (1): Linear(in_features=3584, out_features=896, bias=True)\n",
            "            )\n",
            "            (act): GELU(approximate='none')\n",
            "          )\n",
            "          (proj): Linear(in_features=448, out_features=896, bias=True)\n",
            "        )\n",
            "        (22-23): 2 x MultiScaleBlock(\n",
            "          (norm1): LayerNorm((896,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): MultiScaleAttention(\n",
            "            (qkv): Linear(in_features=896, out_features=2688, bias=True)\n",
            "            (proj): Linear(in_features=896, out_features=896, bias=True)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((896,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLP(\n",
            "            (layers): ModuleList(\n",
            "              (0): Linear(in_features=896, out_features=3584, bias=True)\n",
            "              (1): Linear(in_features=3584, out_features=896, bias=True)\n",
            "            )\n",
            "            (act): GELU(approximate='none')\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (neck): FpnNeck(\n",
            "      (position_encoding): PositionEmbeddingSine()\n",
            "      (convs): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (conv): Conv2d(896, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (conv): Conv2d(448, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (conv): Conv2d(224, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (3): Sequential(\n",
            "          (conv): Conv2d(112, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (mask_downsample): Conv2d(1, 1, kernel_size=(4, 4), stride=(4, 4))\n",
            "  (memory_attention): MemoryAttention(\n",
            "    (layers): ModuleList(\n",
            "      (0-3): 4 x MemoryAttentionLayer(\n",
            "        (self_attn): RoPEAttention(\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (cross_attn_image): RoPEAttention(\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (k_proj): Linear(in_features=64, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=64, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        (dropout3): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (memory_encoder): MemoryEncoder(\n",
            "    (mask_downsampler): MaskDownSampler(\n",
            "      (encoder): Sequential(\n",
            "        (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (1): LayerNorm2d()\n",
            "        (2): GELU(approximate='none')\n",
            "        (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (4): LayerNorm2d()\n",
            "        (5): GELU(approximate='none')\n",
            "        (6): Conv2d(16, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (7): LayerNorm2d()\n",
            "        (8): GELU(approximate='none')\n",
            "        (9): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (10): LayerNorm2d()\n",
            "        (11): GELU(approximate='none')\n",
            "        (12): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (pix_feat_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fuser): Fuser(\n",
            "      (proj): Identity()\n",
            "      (layers): ModuleList(\n",
            "        (0-1): 2 x CXBlock(\n",
            "          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
            "          (norm): LayerNorm2d()\n",
            "          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (position_encoding): PositionEmbeddingSine()\n",
            "    (out_proj): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (sam_prompt_encoder): PromptEncoder(\n",
            "    (pe_layer): PositionEmbeddingRandom()\n",
            "    (point_embeddings): ModuleList(\n",
            "      (0-3): 4 x Embedding(1, 256)\n",
            "    )\n",
            "    (not_a_point_embed): Embedding(1, 256)\n",
            "    (mask_downscaling): Sequential(\n",
            "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (1): LayerNorm2d()\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (4): LayerNorm2d()\n",
            "      (5): GELU(approximate='none')\n",
            "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (no_mask_embed): Embedding(1, 256)\n",
            "  )\n",
            "  (sam_mask_decoder): MaskDecoder(\n",
            "    (transformer): TwoWayTransformer(\n",
            "      (layers): ModuleList(\n",
            "        (0-1): 2 x TwoWayAttentionBlock(\n",
            "          (self_attn): Attention(\n",
            "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (cross_attn_token_to_image): Attention(\n",
            "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): MLP(\n",
            "            (layers): ModuleList(\n",
            "              (0): Linear(in_features=256, out_features=2048, bias=True)\n",
            "              (1): Linear(in_features=2048, out_features=256, bias=True)\n",
            "            )\n",
            "            (act): ReLU()\n",
            "          )\n",
            "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (cross_attn_image_to_token): Attention(\n",
            "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (final_attn_token_to_image): Attention(\n",
            "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
            "      )\n",
            "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (iou_token): Embedding(1, 256)\n",
            "    (mask_tokens): Embedding(4, 256)\n",
            "    (obj_score_token): Embedding(1, 256)\n",
            "    (output_upscaling): Sequential(\n",
            "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (1): LayerNorm2d()\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (4): GELU(approximate='none')\n",
            "    )\n",
            "    (conv_s0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (conv_s1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (output_hypernetworks_mlps): ModuleList(\n",
            "      (0-3): 4 x MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
            "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
            "        )\n",
            "        (act): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (iou_prediction_head): MLP(\n",
            "      (layers): ModuleList(\n",
            "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
            "      )\n",
            "      (act): ReLU()\n",
            "    )\n",
            "    (pred_obj_score_head): MLP(\n",
            "      (layers): ModuleList(\n",
            "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        (2): Linear(in_features=256, out_features=1, bias=True)\n",
            "      )\n",
            "      (act): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (obj_ptr_proj): MLP(\n",
            "    (layers): ModuleList(\n",
            "      (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "    (act): ReLU()\n",
            "  )\n",
            "  (obj_ptr_tpos_proj): Linear(in_features=256, out_features=64, bias=True)\n",
            ")\n",
            "INFO 2024-12-14 12:18:15,768 trainer.py:1062: \tTotal parameters 80.9 M\n",
            "INFO 2024-12-14 12:18:15,768 trainer.py:1063: \tTrainable parameters 80.9 M\n",
            "INFO 2024-12-14 12:18:15,768 trainer.py:1066: \tNon-Trainable parameters 0  \n",
            "INFO 2024-12-14 12:18:15,769 trainer.py:1069: ====================\n",
            "INFO 2024-12-14 12:18:15,776 trainer.py:1023: Finished setting up components: Model, loss, optim, meters etc.\n",
            "INFO 2024-12-14 12:18:15,776 trainer.py: 314: Moving components to device cuda:0 and local rank 0.\n",
            "INFO 2024-12-14 12:18:15,892 trainer.py: 320: Done moving components to device cuda:0 and local rank 0.\n",
            "INFO 2024-12-14 12:18:15,909 optimizer.py: 248: Matches for param_name [image_encoder.*]: {'image_encoder.trunk.blocks.6.mlp.layers.1.weight', 'image_encoder.trunk.blocks.2.attn.proj.weight', 'image_encoder.trunk.blocks.4.mlp.layers.0.bias', 'image_encoder.trunk.blocks.13.norm2.weight', 'image_encoder.trunk.blocks.11.norm1.weight', 'image_encoder.trunk.blocks.3.attn.proj.bias', 'image_encoder.trunk.blocks.4.norm1.bias', 'image_encoder.trunk.blocks.17.attn.proj.weight', 'image_encoder.trunk.blocks.8.mlp.layers.1.bias', 'image_encoder.trunk.blocks.15.attn.qkv.weight', 'image_encoder.trunk.blocks.2.norm1.bias', 'image_encoder.trunk.blocks.7.mlp.layers.0.bias', 'image_encoder.trunk.blocks.6.attn.proj.bias', 'image_encoder.trunk.blocks.5.proj.weight', 'image_encoder.trunk.blocks.5.norm1.weight', 'image_encoder.trunk.blocks.22.norm2.bias', 'image_encoder.neck.convs.3.conv.weight', 'image_encoder.trunk.blocks.11.mlp.layers.1.weight', 'image_encoder.trunk.blocks.10.norm1.bias', 'image_encoder.trunk.blocks.11.norm2.weight', 'image_encoder.trunk.blocks.20.norm1.weight', 'image_encoder.neck.convs.3.conv.bias', 'image_encoder.trunk.blocks.16.norm2.weight', 'image_encoder.trunk.blocks.3.mlp.layers.1.bias', 'image_encoder.trunk.blocks.19.norm1.weight', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.21.norm2.bias', 'image_encoder.trunk.blocks.18.mlp.layers.0.weight', 'image_encoder.trunk.blocks.21.norm2.weight', 'image_encoder.trunk.blocks.12.attn.proj.bias', 'image_encoder.trunk.blocks.20.mlp.layers.0.weight', 'image_encoder.trunk.blocks.16.norm2.bias', 'image_encoder.trunk.blocks.14.mlp.layers.1.weight', 'image_encoder.trunk.blocks.1.norm1.weight', 'image_encoder.trunk.blocks.5.norm2.bias', 'image_encoder.trunk.blocks.4.mlp.layers.1.bias', 'image_encoder.trunk.blocks.0.attn.proj.bias', 'image_encoder.trunk.blocks.14.attn.proj.weight', 'image_encoder.trunk.blocks.7.mlp.layers.1.bias', 'image_encoder.trunk.blocks.4.norm1.weight', 'image_encoder.trunk.blocks.15.norm2.bias', 'image_encoder.trunk.blocks.8.norm1.bias', 'image_encoder.trunk.blocks.3.mlp.layers.0.bias', 'image_encoder.trunk.blocks.23.norm2.bias', 'image_encoder.trunk.blocks.0.mlp.layers.1.bias', 'image_encoder.trunk.blocks.10.mlp.layers.0.weight', 'image_encoder.trunk.blocks.20.attn.proj.bias', 'image_encoder.trunk.blocks.6.norm2.bias', 'image_encoder.trunk.blocks.11.attn.qkv.weight', 'image_encoder.trunk.blocks.10.norm2.weight', 'image_encoder.trunk.blocks.5.attn.qkv.weight', 'image_encoder.trunk.blocks.10.mlp.layers.0.bias', 'image_encoder.trunk.blocks.14.attn.qkv.weight', 'image_encoder.trunk.blocks.20.attn.qkv.weight', 'image_encoder.trunk.blocks.4.attn.proj.bias', 'image_encoder.trunk.blocks.0.attn.proj.weight', 'image_encoder.trunk.blocks.23.mlp.layers.0.bias', 'image_encoder.trunk.blocks.12.norm1.bias', 'image_encoder.trunk.blocks.23.attn.proj.bias', 'image_encoder.trunk.blocks.9.norm2.weight', 'image_encoder.trunk.blocks.14.mlp.layers.1.bias', 'image_encoder.trunk.blocks.5.mlp.layers.0.bias', 'image_encoder.trunk.blocks.7.norm1.bias', 'image_encoder.trunk.blocks.22.attn.qkv.bias', 'image_encoder.trunk.blocks.5.mlp.layers.1.bias', 'image_encoder.trunk.blocks.4.attn.qkv.weight', 'image_encoder.trunk.blocks.2.norm2.bias', 'image_encoder.trunk.blocks.14.attn.proj.bias', 'image_encoder.trunk.blocks.20.mlp.layers.0.bias', 'image_encoder.trunk.blocks.0.mlp.layers.1.weight', 'image_encoder.trunk.blocks.17.norm2.bias', 'image_encoder.trunk.blocks.15.attn.proj.weight', 'image_encoder.trunk.blocks.2.mlp.layers.1.bias', 'image_encoder.trunk.blocks.23.norm1.weight', 'image_encoder.trunk.blocks.18.mlp.layers.1.weight', 'image_encoder.trunk.blocks.8.attn.qkv.bias', 'image_encoder.trunk.blocks.16.attn.qkv.bias', 'image_encoder.trunk.blocks.17.attn.qkv.weight', 'image_encoder.trunk.blocks.1.mlp.layers.0.weight', 'image_encoder.trunk.blocks.5.norm1.bias', 'image_encoder.trunk.blocks.13.mlp.layers.0.weight', 'image_encoder.trunk.blocks.22.attn.proj.bias', 'image_encoder.trunk.blocks.13.mlp.layers.1.bias', 'image_encoder.trunk.blocks.4.attn.qkv.bias', 'image_encoder.trunk.blocks.12.mlp.layers.0.weight', 'image_encoder.trunk.blocks.5.mlp.layers.0.weight', 'image_encoder.trunk.blocks.14.norm2.weight', 'image_encoder.trunk.blocks.5.mlp.layers.1.weight', 'image_encoder.trunk.blocks.11.attn.proj.weight', 'image_encoder.trunk.blocks.21.attn.proj.bias', 'image_encoder.trunk.blocks.3.mlp.layers.1.weight', 'image_encoder.trunk.blocks.12.attn.proj.weight', 'image_encoder.trunk.blocks.17.mlp.layers.0.weight', 'image_encoder.trunk.blocks.1.attn.qkv.bias', 'image_encoder.trunk.blocks.18.mlp.layers.1.bias', 'image_encoder.trunk.blocks.23.norm2.weight', 'image_encoder.trunk.blocks.8.mlp.layers.0.bias', 'image_encoder.trunk.blocks.4.attn.proj.weight', 'image_encoder.trunk.blocks.9.attn.proj.weight', 'image_encoder.trunk.blocks.1.mlp.layers.1.weight', 'image_encoder.trunk.blocks.6.mlp.layers.0.weight', 'image_encoder.trunk.blocks.22.mlp.layers.1.bias', 'image_encoder.trunk.blocks.5.attn.proj.weight', 'image_encoder.trunk.blocks.21.proj.weight', 'image_encoder.trunk.blocks.21.attn.proj.weight', 'image_encoder.trunk.blocks.1.attn.proj.weight', 'image_encoder.trunk.blocks.3.attn.proj.weight', 'image_encoder.trunk.blocks.19.attn.qkv.bias', 'image_encoder.neck.convs.0.conv.bias', 'image_encoder.trunk.blocks.12.attn.qkv.weight', 'image_encoder.trunk.blocks.18.attn.proj.bias', 'image_encoder.trunk.blocks.2.proj.weight', 'image_encoder.trunk.blocks.22.mlp.layers.0.bias', 'image_encoder.trunk.blocks.13.attn.proj.weight', 'image_encoder.neck.convs.0.conv.weight', 'image_encoder.trunk.blocks.7.mlp.layers.0.weight', 'image_encoder.trunk.blocks.8.norm2.bias', 'image_encoder.trunk.blocks.2.proj.bias', 'image_encoder.trunk.blocks.21.attn.qkv.bias', 'image_encoder.trunk.blocks.16.mlp.layers.0.bias', 'image_encoder.trunk.blocks.13.norm1.bias', 'image_encoder.trunk.blocks.23.mlp.layers.1.bias', 'image_encoder.trunk.blocks.6.attn.proj.weight', 'image_encoder.trunk.blocks.1.attn.qkv.weight', 'image_encoder.trunk.blocks.8.attn.proj.bias', 'image_encoder.trunk.blocks.21.proj.bias', 'image_encoder.trunk.blocks.16.mlp.layers.1.bias', 'image_encoder.trunk.blocks.14.norm2.bias', 'image_encoder.trunk.blocks.18.attn.proj.weight', 'image_encoder.trunk.blocks.3.norm2.weight', 'image_encoder.trunk.blocks.8.attn.qkv.weight', 'image_encoder.trunk.blocks.18.norm1.bias', 'image_encoder.trunk.blocks.2.mlp.layers.0.bias', 'image_encoder.trunk.blocks.1.attn.proj.bias', 'image_encoder.trunk.blocks.21.mlp.layers.0.weight', 'image_encoder.trunk.blocks.6.mlp.layers.0.bias', 'image_encoder.trunk.blocks.14.norm1.bias', 'image_encoder.trunk.blocks.7.attn.qkv.weight', 'image_encoder.trunk.blocks.21.mlp.layers.1.bias', 'image_encoder.trunk.patch_embed.proj.bias', 'image_encoder.trunk.blocks.22.attn.qkv.weight', 'image_encoder.trunk.blocks.16.attn.qkv.weight', 'image_encoder.trunk.blocks.1.norm2.weight', 'image_encoder.trunk.blocks.6.norm1.weight', 'image_encoder.trunk.blocks.6.norm2.weight', 'image_encoder.trunk.blocks.11.mlp.layers.1.bias', 'image_encoder.trunk.blocks.22.norm1.weight', 'image_encoder.trunk.blocks.1.norm1.bias', 'image_encoder.trunk.blocks.17.mlp.layers.1.weight', 'image_encoder.trunk.blocks.13.norm2.bias', 'image_encoder.trunk.blocks.20.attn.qkv.bias', 'image_encoder.trunk.blocks.2.norm2.weight', 'image_encoder.trunk.blocks.23.attn.proj.weight', 'image_encoder.trunk.blocks.2.attn.qkv.weight', 'image_encoder.trunk.blocks.18.attn.qkv.bias', 'image_encoder.trunk.blocks.2.mlp.layers.0.weight', 'image_encoder.trunk.blocks.4.mlp.layers.0.weight', 'image_encoder.trunk.blocks.9.norm2.bias', 'image_encoder.trunk.blocks.14.mlp.layers.0.bias', 'image_encoder.trunk.blocks.18.mlp.layers.0.bias', 'image_encoder.trunk.blocks.15.mlp.layers.0.bias', 'image_encoder.trunk.blocks.20.norm2.bias', 'image_encoder.trunk.blocks.4.norm2.weight', 'image_encoder.neck.convs.1.conv.weight', 'image_encoder.trunk.blocks.10.norm2.bias', 'image_encoder.trunk.blocks.11.mlp.layers.0.weight', 'image_encoder.neck.convs.2.conv.bias', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.trunk.blocks.19.norm1.bias', 'image_encoder.trunk.pos_embed', 'image_encoder.trunk.blocks.19.attn.qkv.weight', 'image_encoder.trunk.blocks.17.norm1.bias', 'image_encoder.trunk.blocks.23.mlp.layers.1.weight', 'image_encoder.trunk.blocks.1.mlp.layers.0.bias', 'image_encoder.trunk.blocks.19.attn.proj.weight', 'image_encoder.trunk.blocks.15.norm1.weight', 'image_encoder.trunk.blocks.17.mlp.layers.0.bias', 'image_encoder.trunk.blocks.15.attn.qkv.bias', 'image_encoder.trunk.blocks.17.norm1.weight', 'image_encoder.trunk.blocks.10.norm1.weight', 'image_encoder.trunk.blocks.23.norm1.bias', 'image_encoder.trunk.blocks.9.norm1.bias', 'image_encoder.trunk.blocks.13.attn.proj.bias', 'image_encoder.trunk.blocks.19.mlp.layers.1.bias', 'image_encoder.trunk.blocks.5.proj.bias', 'image_encoder.trunk.blocks.16.mlp.layers.1.weight', 'image_encoder.trunk.blocks.12.mlp.layers.1.bias', 'image_encoder.trunk.blocks.3.norm1.weight', 'image_encoder.trunk.blocks.2.mlp.layers.1.weight', 'image_encoder.trunk.blocks.16.mlp.layers.0.weight', 'image_encoder.trunk.blocks.15.mlp.layers.1.bias', 'image_encoder.trunk.blocks.9.mlp.layers.1.bias', 'image_encoder.trunk.blocks.7.norm2.weight', 'image_encoder.trunk.blocks.11.attn.qkv.bias', 'image_encoder.trunk.blocks.13.norm1.weight', 'image_encoder.trunk.blocks.8.norm2.weight', 'image_encoder.trunk.blocks.12.norm1.weight', 'image_encoder.trunk.blocks.10.attn.proj.weight', 'image_encoder.trunk.blocks.23.attn.qkv.bias', 'image_encoder.trunk.blocks.15.mlp.layers.0.weight', 'image_encoder.trunk.blocks.23.attn.qkv.weight', 'image_encoder.trunk.blocks.7.norm1.weight', 'image_encoder.trunk.blocks.9.attn.qkv.weight', 'image_encoder.trunk.blocks.16.attn.proj.weight', 'image_encoder.trunk.blocks.23.mlp.layers.0.weight', 'image_encoder.trunk.blocks.12.mlp.layers.1.weight', 'image_encoder.trunk.blocks.18.norm2.bias', 'image_encoder.trunk.blocks.7.mlp.layers.1.weight', 'image_encoder.trunk.blocks.22.mlp.layers.1.weight', 'image_encoder.trunk.blocks.11.mlp.layers.0.bias', 'image_encoder.neck.convs.2.conv.weight', 'image_encoder.trunk.patch_embed.proj.weight', 'image_encoder.trunk.blocks.16.norm1.bias', 'image_encoder.trunk.blocks.21.attn.qkv.weight', 'image_encoder.trunk.blocks.8.attn.proj.weight', 'image_encoder.trunk.blocks.19.norm2.weight', 'image_encoder.trunk.blocks.3.attn.qkv.bias', 'image_encoder.trunk.blocks.11.attn.proj.bias', 'image_encoder.trunk.blocks.9.mlp.layers.1.weight', 'image_encoder.trunk.blocks.16.attn.proj.bias', 'image_encoder.trunk.blocks.4.norm2.bias', 'image_encoder.trunk.blocks.9.attn.qkv.bias', 'image_encoder.trunk.blocks.5.attn.qkv.bias', 'image_encoder.trunk.blocks.19.mlp.layers.0.weight', 'image_encoder.trunk.blocks.5.norm2.weight', 'image_encoder.trunk.blocks.3.mlp.layers.0.weight', 'image_encoder.trunk.blocks.17.attn.proj.bias', 'image_encoder.trunk.blocks.21.norm1.bias', 'image_encoder.trunk.blocks.4.mlp.layers.1.weight', 'image_encoder.trunk.blocks.17.mlp.layers.1.bias', 'image_encoder.trunk.blocks.17.attn.qkv.bias', 'image_encoder.trunk.blocks.3.attn.qkv.weight', 'image_encoder.trunk.blocks.19.attn.proj.bias', 'image_encoder.trunk.blocks.20.norm2.weight', 'image_encoder.trunk.blocks.8.mlp.layers.1.weight', 'image_encoder.trunk.blocks.18.attn.qkv.weight', 'image_encoder.trunk.blocks.0.norm2.bias', 'image_encoder.trunk.blocks.10.attn.proj.bias', 'image_encoder.trunk.blocks.8.mlp.layers.0.weight', 'image_encoder.trunk.blocks.2.attn.qkv.bias', 'image_encoder.trunk.blocks.0.attn.qkv.bias', 'image_encoder.trunk.pos_embed_window', 'image_encoder.trunk.blocks.18.norm1.weight', 'image_encoder.trunk.blocks.13.attn.qkv.bias', 'image_encoder.trunk.blocks.7.attn.proj.bias', 'image_encoder.trunk.blocks.0.mlp.layers.0.weight', 'image_encoder.trunk.blocks.11.norm1.bias', 'image_encoder.trunk.blocks.15.attn.proj.bias', 'image_encoder.trunk.blocks.14.norm1.weight', 'image_encoder.trunk.blocks.21.norm1.weight', 'image_encoder.trunk.blocks.10.mlp.layers.1.weight', 'image_encoder.trunk.blocks.10.attn.qkv.bias', 'image_encoder.trunk.blocks.12.attn.qkv.bias', 'image_encoder.trunk.blocks.13.attn.qkv.weight', 'image_encoder.trunk.blocks.20.mlp.layers.1.weight', 'image_encoder.trunk.blocks.6.mlp.layers.1.bias', 'image_encoder.trunk.blocks.18.norm2.weight', 'image_encoder.trunk.blocks.2.norm1.weight', 'image_encoder.trunk.blocks.21.mlp.layers.1.weight', 'image_encoder.trunk.blocks.22.norm2.weight', 'image_encoder.trunk.blocks.15.norm2.weight', 'image_encoder.trunk.blocks.6.attn.qkv.bias', 'image_encoder.trunk.blocks.20.mlp.layers.1.bias', 'image_encoder.trunk.blocks.22.mlp.layers.0.weight', 'image_encoder.trunk.blocks.13.mlp.layers.1.weight', 'image_encoder.trunk.blocks.19.mlp.layers.1.weight', 'image_encoder.trunk.blocks.19.mlp.layers.0.bias', 'image_encoder.trunk.blocks.12.mlp.layers.0.bias', 'image_encoder.trunk.blocks.6.norm1.bias', 'image_encoder.trunk.blocks.20.attn.proj.weight', 'image_encoder.trunk.blocks.0.mlp.layers.0.bias', 'image_encoder.trunk.blocks.8.norm1.weight', 'image_encoder.trunk.blocks.0.norm2.weight', 'image_encoder.trunk.blocks.10.attn.qkv.weight', 'image_encoder.trunk.blocks.7.attn.proj.weight', 'image_encoder.trunk.blocks.9.norm1.weight', 'image_encoder.trunk.blocks.9.mlp.layers.0.weight', 'image_encoder.trunk.blocks.10.mlp.layers.1.bias', 'image_encoder.trunk.blocks.21.mlp.layers.0.bias', 'image_encoder.trunk.blocks.12.norm2.weight', 'image_encoder.trunk.blocks.6.attn.qkv.weight', 'image_encoder.trunk.blocks.0.norm1.bias', 'image_encoder.trunk.blocks.9.attn.proj.bias', 'image_encoder.trunk.blocks.0.attn.qkv.weight', 'image_encoder.trunk.blocks.3.norm2.bias', 'image_encoder.trunk.blocks.17.norm2.weight', 'image_encoder.trunk.blocks.15.mlp.layers.1.weight', 'image_encoder.trunk.blocks.14.attn.qkv.bias', 'image_encoder.trunk.blocks.13.mlp.layers.0.bias', 'image_encoder.trunk.blocks.7.norm2.bias', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.22.attn.proj.weight', 'image_encoder.trunk.blocks.16.norm1.weight', 'image_encoder.trunk.blocks.20.norm1.bias', 'image_encoder.trunk.blocks.22.norm1.bias', 'image_encoder.trunk.blocks.14.mlp.layers.0.weight', 'image_encoder.trunk.blocks.5.attn.proj.bias', 'image_encoder.trunk.blocks.19.norm2.bias', 'image_encoder.trunk.blocks.1.mlp.layers.1.bias', 'image_encoder.trunk.blocks.0.norm1.weight', 'image_encoder.trunk.blocks.15.norm1.bias', 'image_encoder.trunk.blocks.2.attn.proj.bias', 'image_encoder.trunk.blocks.3.norm1.bias', 'image_encoder.neck.convs.1.conv.bias', 'image_encoder.trunk.blocks.7.attn.qkv.bias', 'image_encoder.trunk.blocks.9.mlp.layers.0.bias'}\n",
            "INFO 2024-12-14 12:18:15,974 optimizer.py: 248: Matches for param_name [*bias*]: {'memory_encoder.fuser.layers.0.norm.bias', 'memory_attention.layers.2.cross_attn_image.out_proj.bias', 'memory_encoder.fuser.layers.1.norm.bias', 'memory_attention.layers.0.self_attn.k_proj.bias', 'memory_attention.layers.1.norm1.bias', 'image_encoder.trunk.blocks.4.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias', 'image_encoder.trunk.blocks.3.attn.proj.bias', 'image_encoder.trunk.blocks.4.norm1.bias', 'sam_prompt_encoder.mask_downscaling.6.bias', 'memory_attention.layers.1.cross_attn_image.k_proj.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias', 'image_encoder.trunk.blocks.8.mlp.layers.1.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.0.bias', 'image_encoder.trunk.blocks.2.norm1.bias', 'image_encoder.trunk.blocks.7.mlp.layers.0.bias', 'memory_attention.norm.bias', 'image_encoder.trunk.blocks.6.attn.proj.bias', 'memory_encoder.mask_downsampler.encoder.7.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.out_proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias', 'sam_mask_decoder.transformer.layers.1.norm2.bias', 'image_encoder.trunk.blocks.22.norm2.bias', 'memory_attention.layers.1.cross_attn_image.q_proj.bias', 'image_encoder.trunk.blocks.10.norm1.bias', 'image_encoder.neck.convs.3.conv.bias', 'memory_attention.layers.2.self_attn.k_proj.bias', 'memory_encoder.pix_feat_proj.bias', 'memory_attention.layers.0.cross_attn_image.k_proj.bias', 'memory_encoder.fuser.layers.0.dwconv.bias', 'image_encoder.trunk.blocks.3.mlp.layers.1.bias', 'memory_attention.layers.3.self_attn.v_proj.bias', 'sam_mask_decoder.output_upscaling.3.bias', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.21.norm2.bias', 'image_encoder.trunk.blocks.7.attn.qkv.bias', 'memory_attention.layers.0.linear1.bias', 'memory_attention.layers.0.norm3.bias', 'image_encoder.trunk.blocks.12.attn.proj.bias', 'memory_attention.layers.2.cross_attn_image.v_proj.bias', 'image_encoder.trunk.blocks.16.norm2.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.v_proj.bias', 'image_encoder.trunk.blocks.5.norm2.bias', 'image_encoder.trunk.blocks.4.mlp.layers.1.bias', 'image_encoder.trunk.blocks.0.attn.proj.bias', 'memory_encoder.out_proj.bias', 'image_encoder.trunk.blocks.7.mlp.layers.1.bias', 'memory_attention.layers.3.cross_attn_image.out_proj.bias', 'sam_mask_decoder.transformer.layers.1.norm3.bias', 'image_encoder.trunk.blocks.15.norm2.bias', 'image_encoder.trunk.blocks.3.mlp.layers.0.bias', 'image_encoder.trunk.blocks.8.norm1.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.1.bias', 'image_encoder.trunk.blocks.23.norm2.bias', 'image_encoder.trunk.blocks.0.mlp.layers.1.bias', 'image_encoder.trunk.blocks.20.attn.proj.bias', 'image_encoder.trunk.blocks.6.norm2.bias', 'image_encoder.trunk.blocks.10.mlp.layers.0.bias', 'image_encoder.trunk.blocks.4.attn.proj.bias', 'image_encoder.trunk.blocks.23.mlp.layers.0.bias', 'memory_attention.layers.2.cross_attn_image.k_proj.bias', 'sam_mask_decoder.iou_prediction_head.layers.0.bias', 'image_encoder.trunk.blocks.12.norm1.bias', 'image_encoder.trunk.blocks.23.attn.proj.bias', 'sam_mask_decoder.conv_s0.bias', 'image_encoder.trunk.blocks.5.mlp.layers.0.bias', 'image_encoder.trunk.blocks.14.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias', 'image_encoder.trunk.blocks.7.norm1.bias', 'obj_ptr_tpos_proj.bias', 'image_encoder.trunk.blocks.22.attn.qkv.bias', 'memory_encoder.mask_downsampler.encoder.4.bias', 'image_encoder.trunk.blocks.5.mlp.layers.1.bias', 'memory_attention.layers.2.self_attn.q_proj.bias', 'image_encoder.trunk.blocks.2.norm2.bias', 'image_encoder.trunk.blocks.14.attn.proj.bias', 'image_encoder.trunk.blocks.20.mlp.layers.0.bias', 'image_encoder.trunk.blocks.17.norm2.bias', 'image_encoder.trunk.blocks.2.mlp.layers.1.bias', 'memory_attention.layers.1.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.8.attn.qkv.bias', 'image_encoder.trunk.blocks.16.attn.qkv.bias', 'image_encoder.trunk.blocks.22.attn.proj.bias', 'image_encoder.trunk.blocks.5.norm1.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias', 'image_encoder.trunk.blocks.13.mlp.layers.1.bias', 'image_encoder.trunk.blocks.4.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.1.norm4.bias', 'memory_attention.layers.1.self_attn.k_proj.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.k_proj.bias', 'memory_attention.layers.0.norm1.bias', 'image_encoder.trunk.blocks.21.attn.proj.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias', 'image_encoder.trunk.blocks.1.attn.qkv.bias', 'sam_mask_decoder.output_upscaling.1.bias', 'image_encoder.trunk.blocks.18.mlp.layers.1.bias', 'image_encoder.trunk.blocks.8.mlp.layers.0.bias', 'sam_prompt_encoder.mask_downscaling.0.bias', 'image_encoder.trunk.blocks.22.mlp.layers.1.bias', 'image_encoder.neck.convs.0.conv.bias', 'image_encoder.trunk.blocks.19.attn.qkv.bias', 'memory_attention.layers.1.norm3.bias', 'image_encoder.trunk.blocks.18.attn.proj.bias', 'sam_mask_decoder.transformer.layers.1.mlp.layers.0.bias', 'sam_mask_decoder.pred_obj_score_head.layers.0.bias', 'memory_attention.layers.1.linear1.bias', 'image_encoder.trunk.blocks.22.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.0.bias', 'image_encoder.trunk.blocks.8.norm2.bias', 'image_encoder.trunk.blocks.21.attn.qkv.bias', 'image_encoder.trunk.blocks.2.proj.bias', 'memory_attention.layers.0.self_attn.out_proj.bias', 'memory_attention.layers.3.linear2.bias', 'image_encoder.trunk.blocks.16.mlp.layers.0.bias', 'memory_attention.layers.2.linear1.bias', 'image_encoder.trunk.blocks.23.mlp.layers.1.bias', 'image_encoder.trunk.blocks.13.norm1.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias', 'image_encoder.trunk.blocks.21.proj.bias', 'image_encoder.trunk.blocks.8.attn.proj.bias', 'image_encoder.trunk.blocks.14.norm2.bias', 'image_encoder.trunk.blocks.16.mlp.layers.1.bias', 'image_encoder.trunk.blocks.18.norm1.bias', 'image_encoder.trunk.blocks.1.attn.proj.bias', 'image_encoder.trunk.blocks.2.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.2.bias', 'image_encoder.trunk.blocks.6.mlp.layers.0.bias', 'obj_ptr_proj.layers.0.bias', 'image_encoder.trunk.blocks.21.mlp.layers.1.bias', 'image_encoder.trunk.blocks.14.norm1.bias', 'image_encoder.trunk.patch_embed.proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.1.bias', 'memory_attention.layers.2.norm3.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.k_proj.bias', 'memory_encoder.fuser.layers.1.dwconv.bias', 'image_encoder.trunk.blocks.11.mlp.layers.1.bias', 'image_encoder.trunk.blocks.1.norm1.bias', 'memory_attention.layers.0.cross_attn_image.v_proj.bias', 'mask_downsample.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.13.norm2.bias', 'memory_attention.layers.1.self_attn.q_proj.bias', 'image_encoder.trunk.blocks.18.attn.qkv.bias', 'sam_mask_decoder.transformer.norm_final_attn.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.0.bias', 'image_encoder.trunk.blocks.9.norm2.bias', 'image_encoder.trunk.blocks.14.mlp.layers.0.bias', 'image_encoder.trunk.blocks.18.mlp.layers.0.bias', 'image_encoder.trunk.blocks.15.mlp.layers.0.bias', 'image_encoder.trunk.blocks.20.norm2.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias', 'image_encoder.trunk.blocks.10.norm2.bias', 'sam_mask_decoder.transformer.layers.0.norm1.bias', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.neck.convs.2.conv.bias', 'memory_encoder.mask_downsampler.encoder.1.bias', 'image_encoder.trunk.blocks.19.norm1.bias', 'memory_attention.layers.3.self_attn.q_proj.bias', 'memory_attention.layers.0.norm2.bias', 'memory_encoder.mask_downsampler.encoder.0.bias', 'image_encoder.trunk.blocks.17.norm1.bias', 'sam_mask_decoder.transformer.layers.0.norm4.bias', 'image_encoder.trunk.blocks.1.mlp.layers.0.bias', 'image_encoder.trunk.blocks.15.attn.qkv.bias', 'image_encoder.trunk.blocks.17.mlp.layers.0.bias', 'memory_attention.layers.0.cross_attn_image.out_proj.bias', 'memory_attention.layers.2.norm1.bias', 'sam_prompt_encoder.mask_downscaling.3.bias', 'image_encoder.trunk.blocks.23.norm1.bias', 'image_encoder.trunk.blocks.9.norm1.bias', 'image_encoder.trunk.blocks.13.attn.proj.bias', 'image_encoder.trunk.blocks.19.mlp.layers.1.bias', 'image_encoder.trunk.blocks.5.proj.bias', 'image_encoder.trunk.blocks.12.mlp.layers.1.bias', 'memory_attention.layers.2.cross_attn_image.q_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.2.bias', 'sam_mask_decoder.iou_prediction_head.layers.2.bias', 'memory_attention.layers.1.linear2.bias', 'memory_attention.layers.2.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.15.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.1.norm1.bias', 'image_encoder.trunk.blocks.9.mlp.layers.1.bias', 'image_encoder.trunk.blocks.11.attn.qkv.bias', 'memory_encoder.mask_downsampler.encoder.12.bias', 'image_encoder.trunk.blocks.23.attn.qkv.bias', 'memory_encoder.fuser.layers.0.pwconv1.bias', 'obj_ptr_proj.layers.2.bias', 'memory_attention.layers.2.linear2.bias', 'image_encoder.trunk.blocks.18.norm2.bias', 'memory_attention.layers.0.self_attn.q_proj.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.q_proj.bias', 'image_encoder.trunk.blocks.11.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.0.norm2.bias', 'image_encoder.trunk.blocks.16.norm1.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias', 'memory_encoder.fuser.layers.1.pwconv2.bias', 'image_encoder.trunk.blocks.3.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.0.norm3.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias', 'image_encoder.trunk.blocks.11.attn.proj.bias', 'image_encoder.trunk.blocks.16.attn.proj.bias', 'image_encoder.trunk.blocks.4.norm2.bias', 'image_encoder.trunk.blocks.9.attn.qkv.bias', 'memory_attention.layers.3.cross_attn_image.q_proj.bias', 'image_encoder.trunk.blocks.5.attn.qkv.bias', 'sam_prompt_encoder.mask_downscaling.1.bias', 'image_encoder.trunk.blocks.17.attn.proj.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.out_proj.bias', 'memory_encoder.fuser.layers.0.pwconv2.bias', 'image_encoder.trunk.blocks.21.norm1.bias', 'memory_attention.layers.0.linear2.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias', 'image_encoder.trunk.blocks.17.mlp.layers.1.bias', 'image_encoder.trunk.blocks.17.attn.qkv.bias', 'memory_attention.layers.1.self_attn.out_proj.bias', 'memory_attention.layers.3.norm2.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.19.attn.proj.bias', 'obj_ptr_proj.layers.1.bias', 'memory_attention.layers.3.cross_attn_image.k_proj.bias', 'image_encoder.trunk.blocks.0.norm2.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.q_proj.bias', 'sam_mask_decoder.output_upscaling.0.bias', 'image_encoder.trunk.blocks.2.attn.qkv.bias', 'memory_attention.layers.3.cross_attn_image.v_proj.bias', 'image_encoder.trunk.blocks.10.attn.proj.bias', 'sam_mask_decoder.pred_obj_score_head.layers.2.bias', 'image_encoder.trunk.blocks.0.attn.qkv.bias', 'memory_encoder.mask_downsampler.encoder.6.bias', 'image_encoder.trunk.blocks.13.attn.qkv.bias', 'memory_attention.layers.1.norm2.bias', 'image_encoder.trunk.blocks.7.attn.proj.bias', 'sam_mask_decoder.conv_s1.bias', 'image_encoder.trunk.blocks.11.norm1.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.0.bias', 'memory_attention.layers.3.norm3.bias', 'image_encoder.trunk.blocks.15.attn.proj.bias', 'memory_encoder.fuser.layers.1.pwconv1.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias', 'sam_mask_decoder.transformer.layers.1.mlp.layers.1.bias', 'memory_attention.layers.2.self_attn.out_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.2.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.10.attn.qkv.bias', 'image_encoder.trunk.blocks.12.attn.qkv.bias', 'memory_encoder.mask_downsampler.encoder.10.bias', 'memory_attention.layers.3.norm1.bias', 'memory_attention.layers.0.cross_attn_image.q_proj.bias', 'image_encoder.trunk.blocks.6.mlp.layers.1.bias', 'memory_attention.layers.0.self_attn.v_proj.bias', 'memory_attention.layers.1.cross_attn_image.v_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.1.bias', 'sam_mask_decoder.transformer.layers.0.mlp.layers.0.bias', 'image_encoder.trunk.blocks.20.mlp.layers.1.bias', 'image_encoder.trunk.blocks.6.attn.qkv.bias', 'image_encoder.trunk.blocks.19.mlp.layers.0.bias', 'memory_encoder.mask_downsampler.encoder.3.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.12.mlp.layers.0.bias', 'image_encoder.trunk.blocks.6.norm1.bias', 'image_encoder.trunk.blocks.0.mlp.layers.0.bias', 'memory_attention.layers.3.self_attn.out_proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias', 'memory_attention.layers.3.linear1.bias', 'sam_mask_decoder.transformer.layers.0.mlp.layers.1.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.2.bias', 'image_encoder.trunk.blocks.10.mlp.layers.1.bias', 'image_encoder.trunk.blocks.21.mlp.layers.0.bias', 'sam_mask_decoder.pred_obj_score_head.layers.1.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.1.bias', 'image_encoder.trunk.blocks.0.norm1.bias', 'image_encoder.trunk.blocks.9.attn.proj.bias', 'sam_prompt_encoder.mask_downscaling.4.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.q_proj.bias', 'image_encoder.trunk.blocks.3.norm2.bias', 'memory_attention.layers.1.cross_attn_image.out_proj.bias', 'image_encoder.trunk.blocks.14.attn.qkv.bias', 'sam_mask_decoder.iou_prediction_head.layers.1.bias', 'image_encoder.trunk.blocks.7.norm2.bias', 'image_encoder.trunk.blocks.13.mlp.layers.0.bias', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.20.attn.qkv.bias', 'image_encoder.trunk.blocks.20.norm1.bias', 'image_encoder.trunk.blocks.22.norm1.bias', 'image_encoder.trunk.blocks.5.attn.proj.bias', 'image_encoder.trunk.blocks.2.attn.proj.bias', 'image_encoder.trunk.blocks.19.norm2.bias', 'memory_encoder.mask_downsampler.encoder.9.bias', 'memory_attention.layers.3.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.15.norm1.bias', 'image_encoder.trunk.blocks.1.mlp.layers.1.bias', 'image_encoder.trunk.blocks.3.norm1.bias', 'image_encoder.neck.convs.1.conv.bias', 'memory_attention.layers.2.norm2.bias', 'image_encoder.trunk.blocks.9.mlp.layers.0.bias'}\n",
            "INFO 2024-12-14 12:18:16,072 optimizer.py: 220: Matches for module_cls_name [torch.nn.LayerNorm]: {'sam_mask_decoder.transformer.layers.0.norm1.weight', 'memory_attention.layers.1.norm1.bias', 'image_encoder.trunk.blocks.13.norm2.weight', 'image_encoder.trunk.blocks.11.norm1.weight', 'image_encoder.trunk.blocks.4.norm1.bias', 'image_encoder.trunk.blocks.2.norm1.bias', 'memory_attention.norm.bias', 'image_encoder.trunk.blocks.5.norm1.weight', 'sam_mask_decoder.transformer.layers.1.norm2.bias', 'image_encoder.trunk.blocks.22.norm2.bias', 'image_encoder.trunk.blocks.10.norm1.bias', 'image_encoder.trunk.blocks.11.norm2.weight', 'image_encoder.trunk.blocks.20.norm1.weight', 'memory_attention.layers.3.norm2.weight', 'image_encoder.trunk.blocks.16.norm2.weight', 'image_encoder.trunk.blocks.19.norm1.weight', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.21.norm2.bias', 'memory_attention.layers.2.norm1.weight', 'memory_attention.layers.0.norm3.bias', 'sam_mask_decoder.transformer.layers.1.norm1.weight', 'image_encoder.trunk.blocks.21.norm2.weight', 'image_encoder.trunk.blocks.16.norm2.bias', 'image_encoder.trunk.blocks.1.norm1.weight', 'image_encoder.trunk.blocks.5.norm2.bias', 'image_encoder.trunk.blocks.8.norm1.bias', 'image_encoder.trunk.blocks.4.norm1.weight', 'sam_mask_decoder.transformer.layers.1.norm3.bias', 'image_encoder.trunk.blocks.15.norm2.bias', 'memory_attention.layers.1.norm1.weight', 'image_encoder.trunk.blocks.6.norm2.bias', 'image_encoder.trunk.blocks.23.norm2.bias', 'image_encoder.trunk.blocks.10.norm2.weight', 'image_encoder.trunk.blocks.12.norm1.bias', 'image_encoder.trunk.blocks.9.norm2.weight', 'memory_attention.layers.3.norm3.weight', 'image_encoder.trunk.blocks.7.norm1.bias', 'image_encoder.trunk.blocks.2.norm2.bias', 'image_encoder.trunk.blocks.23.norm1.weight', 'image_encoder.trunk.blocks.17.norm2.bias', 'sam_mask_decoder.transformer.layers.1.norm4.weight', 'image_encoder.trunk.blocks.5.norm1.bias', 'sam_mask_decoder.transformer.layers.1.norm4.bias', 'image_encoder.trunk.blocks.14.norm2.weight', 'memory_attention.layers.0.norm1.bias', 'memory_attention.layers.1.norm2.weight', 'image_encoder.trunk.blocks.23.norm2.weight', 'sam_mask_decoder.transformer.norm_final_attn.weight', 'memory_attention.layers.0.norm3.weight', 'sam_mask_decoder.transformer.layers.0.norm4.weight', 'memory_attention.layers.1.norm3.bias', 'memory_attention.layers.1.norm3.weight', 'image_encoder.trunk.blocks.8.norm2.bias', 'image_encoder.trunk.blocks.13.norm1.bias', 'memory_attention.norm.weight', 'image_encoder.trunk.blocks.14.norm2.bias', 'image_encoder.trunk.blocks.3.norm2.weight', 'image_encoder.trunk.blocks.18.norm1.bias', 'image_encoder.trunk.blocks.14.norm1.bias', 'memory_attention.layers.2.norm3.bias', 'image_encoder.trunk.blocks.1.norm2.weight', 'image_encoder.trunk.blocks.6.norm1.weight', 'image_encoder.trunk.blocks.6.norm2.weight', 'image_encoder.trunk.blocks.22.norm1.weight', 'image_encoder.trunk.blocks.1.norm1.bias', 'image_encoder.trunk.blocks.13.norm2.bias', 'image_encoder.trunk.blocks.2.norm2.weight', 'memory_attention.layers.0.norm1.weight', 'sam_mask_decoder.transformer.norm_final_attn.bias', 'image_encoder.trunk.blocks.9.norm2.bias', 'image_encoder.trunk.blocks.4.norm2.weight', 'image_encoder.trunk.blocks.20.norm2.bias', 'image_encoder.trunk.blocks.10.norm2.bias', 'sam_mask_decoder.transformer.layers.0.norm1.bias', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.trunk.blocks.19.norm1.bias', 'memory_attention.layers.0.norm2.bias', 'sam_mask_decoder.transformer.layers.1.norm3.weight', 'sam_mask_decoder.transformer.layers.0.norm2.weight', 'image_encoder.trunk.blocks.17.norm1.bias', 'sam_mask_decoder.transformer.layers.0.norm4.bias', 'sam_mask_decoder.transformer.layers.1.norm2.weight', 'image_encoder.trunk.blocks.15.norm1.weight', 'memory_attention.layers.2.norm1.bias', 'image_encoder.trunk.blocks.17.norm1.weight', 'image_encoder.trunk.blocks.10.norm1.weight', 'image_encoder.trunk.blocks.23.norm1.bias', 'image_encoder.trunk.blocks.9.norm1.bias', 'image_encoder.trunk.blocks.3.norm1.weight', 'memory_attention.layers.0.norm2.weight', 'sam_mask_decoder.transformer.layers.1.norm1.bias', 'image_encoder.trunk.blocks.7.norm2.weight', 'image_encoder.trunk.blocks.12.norm1.weight', 'image_encoder.trunk.blocks.13.norm1.weight', 'image_encoder.trunk.blocks.8.norm2.weight', 'image_encoder.trunk.blocks.7.norm1.weight', 'image_encoder.trunk.blocks.18.norm2.bias', 'sam_mask_decoder.transformer.layers.0.norm2.bias', 'image_encoder.trunk.blocks.16.norm1.bias', 'image_encoder.trunk.blocks.19.norm2.weight', 'sam_mask_decoder.transformer.layers.0.norm3.bias', 'image_encoder.trunk.blocks.4.norm2.bias', 'image_encoder.trunk.blocks.5.norm2.weight', 'sam_mask_decoder.transformer.layers.0.norm3.weight', 'image_encoder.trunk.blocks.21.norm1.bias', 'memory_attention.layers.3.norm2.bias', 'image_encoder.trunk.blocks.20.norm2.weight', 'memory_attention.layers.2.norm3.weight', 'image_encoder.trunk.blocks.0.norm2.bias', 'image_encoder.trunk.blocks.18.norm1.weight', 'memory_attention.layers.1.norm2.bias', 'memory_attention.layers.3.norm3.bias', 'image_encoder.trunk.blocks.11.norm1.bias', 'image_encoder.trunk.blocks.14.norm1.weight', 'image_encoder.trunk.blocks.21.norm1.weight', 'memory_attention.layers.3.norm1.bias', 'image_encoder.trunk.blocks.18.norm2.weight', 'image_encoder.trunk.blocks.2.norm1.weight', 'image_encoder.trunk.blocks.22.norm2.weight', 'image_encoder.trunk.blocks.15.norm2.weight', 'image_encoder.trunk.blocks.6.norm1.bias', 'image_encoder.trunk.blocks.8.norm1.weight', 'image_encoder.trunk.blocks.0.norm2.weight', 'image_encoder.trunk.blocks.9.norm1.weight', 'image_encoder.trunk.blocks.12.norm2.weight', 'image_encoder.trunk.blocks.0.norm1.bias', 'image_encoder.trunk.blocks.3.norm2.bias', 'memory_attention.layers.3.norm1.weight', 'image_encoder.trunk.blocks.17.norm2.weight', 'memory_attention.layers.2.norm2.weight', 'image_encoder.trunk.blocks.7.norm2.bias', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.0.norm1.weight', 'image_encoder.trunk.blocks.16.norm1.weight', 'image_encoder.trunk.blocks.20.norm1.bias', 'image_encoder.trunk.blocks.22.norm1.bias', 'image_encoder.trunk.blocks.19.norm2.bias', 'image_encoder.trunk.blocks.15.norm1.bias', 'image_encoder.trunk.blocks.3.norm1.bias', 'memory_attention.layers.2.norm2.bias'} \n",
            "Raw dataset length = 2274\n",
            "INFO 2024-12-14 12:18:17,137 sam2_datasets.py: 125: Dataset mixing probabilities: [1.0]\n",
            "/content/sam2/training/utils/checkpoint_utils.py:275: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(f, map_location=map_location)\n",
            "INFO 2024-12-14 12:18:17,341 trainer.py: 417: Loading pretrained checkpoint from {'_partial_': True, '_target_': 'training.utils.checkpoint_utils.load_state_dict_into_model', 'strict': True, 'ignore_unexpected_keys': None, 'ignore_missing_keys': None, 'state_dict': {'_target_': 'training.utils.checkpoint_utils.load_checkpoint_and_apply_kernels', 'checkpoint_path': './checkpoints/sam2.1_hiera_base_plus.pt', 'ckpt_state_dict_keys': ['model']}}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n",
            "/content/sam2/training/trainer.py:861: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(\n",
            "INFO 2024-12-14 12:19:03,737 train_utils.py: 271: Train Epoch: [0][   0/2274] | Batch Time: 45.77 (45.77) | Data Time: 40.19 (40.19) | Mem (GB): 9.00 (9.00/9.00) | Time Elapsed: 00d 00h 00m | Losses/train_all_loss: 1.64e+00 (1.64e+00)\n",
            "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n",
            "INFO 2024-12-14 12:19:21,810 train_utils.py: 271: Train Epoch: [0][  10/2274] | Batch Time: 1.81 (5.80) | Data Time: 0.00 (3.65) | Mem (GB): 10.00 (9.82/10.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 1.69e+00 (1.36e+00)\n",
            "INFO 2024-12-14 12:19:40,293 train_utils.py: 271: Train Epoch: [0][  20/2274] | Batch Time: 1.94 (3.92) | Data Time: 0.00 (1.91) | Mem (GB): 10.00 (9.90/10.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 1.47e+00 (1.40e+00)\n",
            "INFO 2024-12-14 12:19:58,997 train_utils.py: 271: Train Epoch: [0][  30/2274] | Batch Time: 1.84 (3.26) | Data Time: 0.00 (1.30) | Mem (GB): 10.00 (9.94/10.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 1.43e+00 (1.41e+00)\n",
            "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n",
            "INFO 2024-12-14 12:20:17,652 train_utils.py: 271: Train Epoch: [0][  40/2274] | Batch Time: 1.64 (2.92) | Data Time: 0.00 (0.98) | Mem (GB): 9.00 (9.90/10.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 5.04e-01 (1.42e+00)\n",
            "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n",
            "INFO 2024-12-14 12:20:36,010 train_utils.py: 271: Train Epoch: [0][  50/2274] | Batch Time: 1.83 (2.71) | Data Time: 0.00 (0.79) | Mem (GB): 10.00 (9.90/10.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 1.34e+00 (1.38e+00)\n",
            "INFO 2024-12-14 12:20:54,374 train_utils.py: 271: Train Epoch: [0][  60/2274] | Batch Time: 1.92 (2.56) | Data Time: 0.00 (0.66) | Mem (GB): 10.00 (9.90/10.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 1.38e+00 (1.46e+00)\n",
            "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n",
            "INFO 2024-12-14 12:21:12,834 train_utils.py: 271: Train Epoch: [0][  70/2274] | Batch Time: 1.85 (2.46) | Data Time: 0.00 (0.57) | Mem (GB): 10.00 (9.90/10.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 3.83e+00 (1.51e+00)\n",
            "INFO 2024-12-14 12:21:31,375 train_utils.py: 271: Train Epoch: [0][  80/2274] | Batch Time: 1.83 (2.39) | Data Time: 0.00 (0.50) | Mem (GB): 10.00 (9.90/10.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 6.51e-01 (1.46e+00)\n",
            "INFO 2024-12-14 12:21:49,393 train_utils.py: 271: Train Epoch: [0][  90/2274] | Batch Time: 1.82 (2.32) | Data Time: 0.00 (0.44) | Mem (GB): 10.00 (9.87/10.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 1.22e+00 (1.44e+00)\n",
            "INFO 2024-12-14 12:22:07,558 train_utils.py: 271: Train Epoch: [0][ 100/2274] | Batch Time: 1.85 (2.27) | Data Time: 0.00 (0.40) | Mem (GB): 10.00 (9.86/10.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 7.85e-01 (1.44e+00)\n",
            "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n",
            "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n",
            "INFO 2024-12-14 12:22:26,107 train_utils.py: 271: Train Epoch: [0][ 110/2274] | Batch Time: 1.90 (2.24) | Data Time: 0.00 (0.36) | Mem (GB): 10.00 (9.87/10.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 1.39e+00 (1.43e+00)\n",
            "INFO 2024-12-14 12:22:44,402 train_utils.py: 271: Train Epoch: [0][ 120/2274] | Batch Time: 1.65 (2.20) | Data Time: 0.00 (0.33) | Mem (GB): 9.00 (9.88/10.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 8.02e-01 (1.40e+00)\n",
            "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n",
            "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n",
            "INFO 2024-12-14 12:23:02,734 train_utils.py: 271: Train Epoch: [0][ 130/2274] | Batch Time: 1.83 (2.17) | Data Time: 0.00 (0.31) | Mem (GB): 10.00 (9.87/10.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 9.13e-01 (1.39e+00)\n",
            "INFO 2024-12-14 12:23:20,866 train_utils.py: 271: Train Epoch: [0][ 140/2274] | Batch Time: 1.83 (2.15) | Data Time: 0.00 (0.29) | Mem (GB): 10.00 (9.86/10.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 1.51e+00 (1.41e+00)\n",
            "INFO 2024-12-14 12:23:39,559 train_utils.py: 271: Train Epoch: [0][ 150/2274] | Batch Time: 1.93 (2.13) | Data Time: 0.01 (0.27) | Mem (GB): 10.00 (9.85/10.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 8.00e-01 (1.40e+00)\n",
            "INFO 2024-12-14 12:23:59,198 train_utils.py: 271: Train Epoch: [0][ 160/2274] | Batch Time: 2.00 (2.12) | Data Time: 0.01 (0.25) | Mem (GB): 10.00 (9.86/10.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 6.67e-01 (1.40e+00)\n",
            "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n",
            "INFO 2024-12-14 12:24:17,652 train_utils.py: 271: Train Epoch: [0][ 170/2274] | Batch Time: 1.84 (2.10) | Data Time: 0.00 (0.24) | Mem (GB): 10.00 (9.86/10.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 1.31e+00 (1.40e+00)\n",
            "INFO 2024-12-14 12:24:36,161 train_utils.py: 271: Train Epoch: [0][ 180/2274] | Batch Time: 1.83 (2.09) | Data Time: 0.00 (0.22) | Mem (GB): 10.00 (9.87/10.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 1.52e+00 (1.40e+00)\n",
            "INFO 2024-12-14 12:24:54,036 train_utils.py: 271: Train Epoch: [0][ 190/2274] | Batch Time: 1.87 (2.07) | Data Time: 0.00 (0.21) | Mem (GB): 10.00 (9.86/10.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 1.15e+00 (1.38e+00)\n",
            "INFO 2024-12-14 12:25:12,288 train_utils.py: 271: Train Epoch: [0][ 200/2274] | Batch Time: 1.89 (2.06) | Data Time: 0.00 (0.20) | Mem (GB): 10.00 (9.85/10.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 9.89e-01 (1.35e+00)\n",
            "INFO 2024-12-14 12:25:30,763 train_utils.py: 271: Train Epoch: [0][ 210/2274] | Batch Time: 1.81 (2.05) | Data Time: 0.00 (0.19) | Mem (GB): 10.00 (9.85/10.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 2.42e+00 (1.36e+00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can visualize the model training graphs with Tensorboard:"
      ],
      "metadata": {
        "id": "_jrYZxJjvCJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --bind_all --logdir ./sam2_logs/"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5kGLPEIJzamJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Model Results\n",
        "\n",
        "With a trained model ready, we can test the model on an image from our test set.\n",
        "\n",
        "To assist with visualizing model predictions, we are going to use Roboflow supervision, an open source computer vision Python package with utilities for working with vision model outputs\n"
      ],
      "metadata": {
        "id": "mRWhds58vF21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supervision -q"
      ],
      "metadata": {
        "id": "jRsjKM_4nJ57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load SAM-2.1"
      ],
      "metadata": {
        "id": "wYBZi6sUvMFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sam2.build_sam import build_sam2\n",
        "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
        "import supervision as sv\n",
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# use bfloat16 for the entire notebook\n",
        "# from Meta notebook\n",
        "torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "if torch.cuda.get_device_properties(0).major >= 8:\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "checkpoint = \"/content/sam2/sam2_logs/configs/train.yaml/checkpoints/checkpoint.pt\"\n",
        "model_cfg = \"configs/sam2.1/sam2.1_hiera_b+.yaml\"\n",
        "sam2 = build_sam2(model_cfg, checkpoint, device=\"cuda\")\n",
        "mask_generator = SAM2AutomaticMaskGenerator(sam2)\n",
        "\n",
        "checkpoint_base = \"/content/sam2/checkpoints/sam2.1_hiera_base_plus.pt\"\n",
        "model_cfg_base = \"configs/sam2.1/sam2.1_hiera_b+.yaml\"\n",
        "sam2_base = build_sam2(model_cfg_base, checkpoint_base, device=\"cuda\")\n",
        "mask_generator_base = SAM2AutomaticMaskGenerator(sam2_base)"
      ],
      "metadata": {
        "id": "rnBwUu9zm3tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Inference on an Image in Automatic Mask Generation Mode"
      ],
      "metadata": {
        "id": "vk139UgNvNU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_set = os.listdir(\"/content/data/valid\")\n",
        "\n",
        "# choose random with .json extension\n",
        "image = random.choice([img for img in validation_set if img.endswith(\".jpg\")])\n",
        "image = os.path.join(\"/content/data/valid\", image)\n",
        "opened_image = np.array(Image.open(image).convert(\"RGB\"))\n",
        "result = mask_generator.generate(opened_image)\n",
        "\n",
        "detections = sv.Detections.from_sam(sam_result=result)\n",
        "\n",
        "mask_annotator = sv.MaskAnnotator(color_lookup = sv.ColorLookup.INDEX)\n",
        "annotated_image = opened_image.copy()\n",
        "annotated_image = mask_annotator.annotate(annotated_image, detections=detections)\n",
        "\n",
        "base_annotator = sv.MaskAnnotator(color_lookup = sv.ColorLookup.INDEX)\n",
        "\n",
        "base_result = mask_generator_base.generate(opened_image)\n",
        "base_detections = sv.Detections.from_sam(sam_result=base_result)\n",
        "base_annotated_image = opened_image.copy()\n",
        "base_annotated_image = base_annotator.annotate(base_annotated_image, detections=base_detections)\n",
        "\n",
        "sv.plot_images_grid(images=[annotated_image, base_annotated_image], titles=[\"Fine-Tuned SAM-2.1\", \"Base SAM-2.1\"], grid_size=(1, 2))"
      ],
      "metadata": {
        "id": "ATfwPV9yryk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exporting Model\n"
      ],
      "metadata": {
        "id": "UepbldD1vSPN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "He8M-oCSBkON"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}